\section{Efficacy of Optimisation and Analysis}
\label{sec:ev_oaa}

\subsection{Left Recursion Results}
\label{ssec:lrec_res}

While it is difficult to quantify the effectiveness of rewriting a left-recursive production, it is beneficial to see how real examples of parsers may be transformed.
Note that prior to analysis (if the operators were converted into functions, but no further processing was done), a left-recursive production will still infinitely run until it exceeds the capacity of the call stack.
The same behaviour can be seen in other parser combinator libraries, such as \textit{ts-parsec} and \textit{parsimmon} (TypeScript / JavaScript based libraries, which are compared further in \autoref{sec:ts_compare}).

Consider a simple calculator, which allows for left-associative addition and subtraction of numbers.
The raw (\tsext) code is shown in \autoref{lst:pmcalc_before}, where left recursion is present in two of the three disjunctions for \texttt{calc} and the non-left-recursive disjunction is \texttt{nat}.
It is also important to note that the left-recursive disjunctions (for addition and subtraction) are in different forms.
Note that numerous other forms can be processed due to the normalisation and reduction steps highlighted in \autoref{ssec:lrec_rewrite}

\begin{capminted}
    \begin{minted}{teaspoon_lex.py:TeaspoonLexer -x}
        val nat   = ((s: string) => parseInt(s)) <$> /[0-9]+/;
        lazy calc = (x => y => x + y) <$> (calc <* '+') <*> nat <|>
                    calc <**> ((x => y => x - y) <$ '-') <*> nat <|>
                    nat;
    \end{minted}
    \vspace{-0.5\baselineskip}
    \caption{Simple addition and subtraction calculator, prior to processing}
    \label{lst:pmcalc_before}
\end{capminted}

This code shown in \autoref{lst:pmcalc_after} is generated by the preprocessor from the previous example of the left-recursive calculator.
The preprocessor correctly identifies the disjunction without left recursion and uses it as the first parameter for \texttt{postfix}.
On the other hand, the two left-recursive disjunctions were identified, normalised, and reduced, thus leading to the two existing in the same `shape' (as seen in lines 4 and 5).

Prior to the refactoring, attempting to parse \texttt{'1-2-3+4'} would recurse until the call stack was exhausted (as expected).
However, with the rewritten parser, the computation completes with the expected outcome: giving \texttt{0} as the result.

\begin{capminted}
    \begin{minted}{typescript}
        const nat  = fmap((s: string) => parseInt(s), re(/[0-9]+/));
        const calc = lazy(() => postfix(nat,
            choice(
                pamf(apR(chr('-'), nat), (y) => (x) => x - y),
                pamf(apR(chr('+'), nat), (y) => (x) => x + y)
            )
        ));
    \end{minted}
    \vspace{-0.5\baselineskip}
    \caption{Simple addition and subtraction calculator, after processing}
    \label{lst:pmcalc_after}
\end{capminted}

However, the example above only demonstrated cases with direct left recursion (which is significantly more common in actual use \cite{parr14}).
Despite the rarity of the other two cases, discussed in \autoref{ssec:lrec}, it is still important to be able to detect them, even if they cannot be easily fixed.
This can be seen in \autoref{fig:rec_types}, where \texttt{p} and \texttt{q} are indirectly (and mutually) left-recursive, and \texttt{r} contains hidden left recursion.
The case of mutual recursion is not handled by \textit{ANTLR4} (detailed in \autoref{ssec:antlr4all}), where the grammar will not be accepted, whereas the preprocessor will still perform the refactoring but provides a warning to the user.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.175\textwidth}
        \begin{lstlisting}
            P ::= Q x
            Q ::= P
            R ::= $\varepsilon$ R
        \end{lstlisting}
    \end{minipage}
    \hfill
    $\leadsto$
    \hfill
    \begin{minipage}{0.35\textwidth}
        \begin{minted}{teaspoon_lex.py:TeaspoonLexer -x}
            lazy p = q *> x;
            lazy q = p;
            lazy r = pure("") <* r;
        \end{minted}
    \end{minipage}
    \caption{Left: grammar with indirect and hidden left recursion, right: syntactically equivalent parser}
    \label{fig:rec_types}
\end{figure}

The preprocessor is able to detect both of these cases, noting that \texttt{p} and \texttt{q} have disjunctions with `non-local' left recursion.
On the other hand, \texttt{r} is treated as if it had local left recursion, based on the definitions provided in \autoref{ssec:first_sets}, thus being trivially detected.
This demonstrates the preprocessor's ability to refactor grammars that carry semantic information regarding the parse structure, as well as handle cases that cannot be resolved by some parser generators (especially when the \texttt{inline} declaration is used).
However, it is still important to acknowledge that this technique cannot cover every case: soundness is verified (\autoref{chap:soundness}), not completeness.

\subsection{String Trie Results}
\label{ssec:string_trie_res}

In order to quantify the performance improvements granted by the optimisation, a parser is generated that accepts $n$ words, from a shuffled set of the top 1,000 most English common words \cite{ef1000}.
The aim is to demonstrate the performance penalties caused by backtracking; the original parser is simply a number of alternative strings, each wrapped in \texttt{attempt}.
Ordering was performed to prevent the parser from consuming just a prefix (for example, successfully parsing \texttt{in} when the input is \texttt{include}).
On the other hand, the optimised parser is generated from a sequence of strings with the optimisation enabled, thus creating a string parser that requires no backtracking.
A single run for $n$ words consists of constructing a parser that accepts all $n$ words and then parsing each of the words.
For each $n$, 105 runs are completed on each parser with the results of the initial 5 runs being discarded to account for caching or warming up.

The benefits of the optimisation can be seen most clearly on `larger' parsers (ones with more alternatives).
While an increase is expected in the total execution time of these tests (simply due to more words being tested), a more interesting result is visible in \autoref{fig:st_avg}, where the changes to average (per word) execution time can be seen.
For the unoptimised parser, the trend is roughly linear with respect to the number of alternatives.
On the other hand, the time to parse each word in the optimised parser is roughly constant.

\begin{figure}[H]
    \centering
    \import{figs}{figs/string_trie_results_avg.pgf}
    \vspace{-0.5\baselineskip}
    \caption{Average execution time of parsing a single word on each parser}
    \label{fig:st_avg}
\end{figure}

The same result can be seen with the parser's performance on an invalid word (one that is not in the alternatives), as shown in \autoref{fig:st_avg_fail}.
As expected, the time to fail is longer than the time to successfully parse a valid word as it must reach the end of all the alternatives.
However, with the optimised parser, it can `short-circuit' and fail quite quickly as soon as any parse fails, whereas the unoptimised parser requires going through each \texttt{attempt} disjunction.

\begin{figure}[H]
    \centering
    \import{figs}{figs/string_trie_results_avg_fail.pgf}
    \vspace{-0.5\baselineskip}
    \caption{Average execution time of parsing a single invalid word on each parser}
    \label{fig:st_avg_fail}
\end{figure}

\subsection{Backtracking Reduction Results}
\label{ssec:backtrack_res}

A similar result to \autoref{ssec:string_trie_res} can be observed for generalised backtracking.
In order to test a pathological case of how backtracking reduction can be hugely beneficial, parsers were generated to accept $n$ periods (\texttt{'.'}s) followed by a single character from \texttt{'a'} to \texttt{'j'} - a total of 10 parser disjunctions are generated for each $n$ (shown as parser \texttt{p} in \autoref{lst:gen_n_3}).
A single run executes each of the 10 strings against the generated parser.
Similar to the previous experiment, a total of $105$ runs are performed, with the first $5$ being discarded.
For example, when $n = 3$, the following parser is created:

\begin{capminted}
    \begin{minted}{teaspoon_lex.py:TeaspoonLexer -x}
        val p = attempt('.' *> '.' *> '.' *> 'a') <|> ... <|>
                attempt('.' *> '.' *> '.' *> 'j');
        val q = '.' *> '.' *> '.' *> ('a' <|> ... <|> 'j');
    \end{minted}
    \vspace{-0.5\baselineskip}
    \caption{Pathological of a parser that requires backtracking for $n = 3$ (\texttt{p}) and the `optimised' result (\texttt{q})}
    \label{lst:gen_n_3}
\end{capminted}

It is clear that the parser \texttt{p} described in \autoref{lst:gen_n_3} requires backtracking: all disjunctions have a (large) shared prefix.
The only differentiating factor between the disjunctions is the final character, however, work is repeatedly being done to parse the first $n$ periods, then backtracking when the final character does not match.
As there is a common prefix, it is acceptable to factor it out (due to the explicit use of backtracking through \texttt{attempt}s) into the parser \texttt{q}.

\begin{figure}[H]
    \centering
    \import{figs}{figs/gen_backtrack_results.pgf}
    \vspace{-0.5\baselineskip}
    \caption{Average execution time of parsing all 10 strings}
    \label{fig:gen_backtrack_results}
\end{figure}

As expected, a greater benefit is noted when more backtracking is performed, as seen in \autoref{fig:gen_backtrack_results}.
While there is an increase in the parsing time for the optimised parser, it is expected as the strings are longer (therefore, more parsers need to be applied).
However, with a larger string, significantly more work needs to be redone when backtracking.
While this result is shown on an unrealistic parser, it is completely feasible that a similar pattern can be seen in real-world applications (albeit to a significantly lesser extent).
