
\section{Performance}
\label{sec:perf}

First, note that the majority of the performance optimisations will revolve around the results of the JSON parser, detailed further in \autoref{sec:ts_compare}, as this grammar tests numerous library features and also performs well as an indicator of real-world performance.
However, somewhat pathological cases are also observed in order to highlight certain design decisions.

A major point of optimisation lies in functions that consume an arbitrary number of repetitions - namely \texttt{many}, \texttt{many1}, \texttt{postfix}, and similar (including \texttt{chain}s).
With repetition being a common pattern in numerous grammars, as well as \texttt{postfix} being heavily utilised in the removal of left recursion (detailed in \autoref{sec:lrec_analysis}), reducing the performance penalty can bring significant benefits.
With the base implementation, this is done with expensive recursive calls, with no chance of optimisation from the TypeScript compiler.
However, this can trivially be replaced with an accumulator, iteration, and a mutable state.
This optimisation alone gave up to a fourfold performance improvement on grammars that heavily utilise these parsers, such as JSON as seen in \autoref{fig:rec_vs_itr}.

\begin{figure}[H]
    \centering
    \import{figs}{figs/rec_vs_itr.pgf}
    \vspace{-0.5\baselineskip}
    \caption{Normalised execution time of an iterative implementation (I) versus a recursive implementation (R)}
    \label{fig:rec_vs_itr}
\end{figure}

Recall the previous implementation of laziness resulted in the parser being `generated' for each use.
This leads to an obvious inefficiency - it does not make sense for the parser to be regenerated as long as the generating function is pure (no side effects).
While no guarantees can be made about the `thunk', the general, non-pathological use case of this feature will have pure functions.
In general, any calls to \texttt{lazy} will typically be generated by the preprocessor rather than by the user.
As such, it is possible to store the result of the thunk after it is called for the first time, thus implementing sharing.

\begin{figure}[H]
    \centering
    \import{figs}{figs/old_vs_new.pgf}
    \vspace{-0.5\baselineskip}
    \caption{Normalised execution time of laziness with sharing (S) versus laziness with no sharing (N)}
    \label{fig:share_vs_no}
\end{figure}

When the \texttt{lazy} function is called, an instance of the \texttt{\_Lazy} class is created, which contains two properties; \texttt{\_f} (the thunk) and \texttt{\_p} (the actual `generated' parser) which is initially \texttt{undefined}.
Once an attempt is made to access \texttt{\_p}, the stored value is checked on whether it is defined or not.
If it is defined, the value is simply returned, otherwise the thunk is executed and the value stored.
While the creation of classes can create some minor overhead, the optimisation still results in an overall performance uplift.
This is visible in \autoref{fig:share_vs_no}, where an improvement of $\approx 25\%$ on execution times can be observed.
It is important to note that this result is heavily dependent on the grammar and the amount of laziness required.

Another notable change made from the port is the use of uncurried functions over the use of curried functions.
While there is a very minor performance improvement from using uncurried functions, the main advantage of uncurried functions over curried functions is the fact that TypeScript's type inference occasionally fails to obtain the type for a curried function, but not for the uncurried version.
This issue can be seen in \autoref{lst:fail_type}, where the type of \texttt{u} is successfully inferred to be \texttt{Parser<number>}, whereas the type of \texttt{c} fails to infer and becomes a \texttt{Parser<unknown>}.

\begin{capminted}
    \begin{minted}{typescript}
        let ds = many1(satisfy((c: string) => c >= '0' && c <= '9'));
        let d2n = (ds: string[]) => parseInt(ds.join(''));

        let c = pamf_c(ds)(d2n); // failure: 'Parser<unknown>'
        let u = pamf_u(ds, d2n); // success: 'Parser<number>'
    \end{minted}
    \vspace{-0.5\baselineskip}
    \caption{Example of failed type inference on a simple natural number parser}
    \label{lst:fail_type}
\end{capminted}

While this is a minor annoyance for a user who is manually writing the parsers (as the development environment will likely warn them), it is significantly more problematic when the function calls are being automatically generated.
This issue is caused by TypeScript's inability to infer the function's overall type, when type information is required from the second `call'.
In the example above, \texttt{pamf\_c<A, B>} requires the type of \texttt{d2n} to obtain the type \texttt{B}.
If the example instead used \texttt{fmap\_c(d2n)(ds)}, inference would succeed as both \texttt{A} and \texttt{B} of \texttt{fmap\_c<A, B>} are provided by \texttt{d2n}.
